# Text Summarization Project
## Project Overview
This project involves text summarization using an extractive approach. We aim to summarize a dataset (`dataset.csv`) by selecting the top 3 and top 5 sentences based on sentence scores. The model's performance is evaluated using the ROUGE score, and an analysis of the generated summaries is provided.

## Dataset
The dataset contains the following columns:
- Unnamed: 0: This column appears to be an index or identifier for each row but may not be essential for processing the text. It could be removed or renamed during data cleaning.
- content: This column contains the full text or content of news articles or documents. This is the input text that will be summarized using extractive summarization.
- summary: This column contains a reference summary for each article in the content column.

## Approach
We implement an extractive summarization approach, which involves the following steps:
1. **Data Loading:** Load the dataset from `dataset.csv`, ensuring that the text is properly parsed for summarization.
2. **Text Preprocessing:** Clean and preprocess the text data by removing unwanted characters, punctuation, stop words, and other noise.
3. **Sentence Scoring:** Assign a score to each sentence within a document based on criteria like word frequency, sentence length, or term importance.
4. **Top-n Sentence Selection:** For each document, select the top 3 and top 5 sentences based on their scores.
5. **Summarization:** Combine the selected sentences into a final extractive summary for each document.
6. **Evaluation using ROUGE Score:** Evaluate the quality of the summaries by comparing them to reference summaries using ROUGE metrics.

## Evaluation Metrics
The performance of the summarization model is evaluated using the following ROUGE scores:
- **ROUGE-1:** Measures the overlap of individual words between the predicted summary and the reference summary.
- **ROUGE-2:** Measures the overlap of bigrams (two-word sequences) between the predicted and reference summaries.
- **ROUGE-L:** Evaluates the longest common subsequence, assessing how well the predicted summary maintains the sentence structure of the original text.

## Result and Analysis
Once the model generates the summaries, they are evaluated using the ROUGE score. The following insights are obtained from the analysis:
- ROUGE-1 provides a measure of how much the generated summaries match the individual words of the reference summaries.
- ROUGE-2 helps determine the coherence by measuring how well two consecutive words match between the summaries.
- ROUGE-L captures how closely the structure of the predicted summary matches the reference summary, in terms of the longest subsequence.

## Summary Quality
- Top 3 vs. Top 5 Sentences: We compare the quality of summaries generated by selecting the top 3 versus the top 5 sentences. Typically, selecting more sentences improves recall but may reduce precision.
- Coherence: Does the generated summary maintain the flow and coherence of the original document?

## Conclusion
The extractive summarization model provides concise summaries by selecting high-scoring sentences from the original text. While this method is efficient, it may not always capture the nuanced information present in the original text. The ROUGE evaluation highlights both the strengths and limitations of the extractive approach.
